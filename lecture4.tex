\ifx\pdfminorversion\undefined\else\pdfminorversion=4\fi
\documentclass[aspectratio=169,t]{beamer}
%\documentclass[aspectratio=169,t,handout]{beamer}

% English version FAU Logo
\usepackage[english]{babel}
% German version FAU Logo
%\usepackage[ngerman]{babel}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fontawesome}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{pgfplots,pgfplotstable,pgf-pie}
\usepackage{filecontents}
\newcommand{\plots}{0.611201}
\newcommand{\plotm}{2.19882}
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\tikzset{
    vertex/.style = {
        circle,
        fill            = black,
        outer sep = 2pt,
        inner sep = 1pt,
    }
}
\usetikzlibrary{matrix,mindmap}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,shapes.symbols,chains,intersections,snakes,positioning}
\tikzset{level 1/.append style={sibling angle=50,level distance = 165mm}}
\tikzset{level 2/.append style={sibling angle=20,level distance = 45mm}}
\tikzset{every node/.append style={scale=1}}
% read in data file
\pgfplotstableread{data/iris.dat}\iris
% get number of data points
\pgfplotstablegetrowsof{\iris}
\pgfmathsetmacro\NumRows{\pgfplotsretval-1}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.14}
\newcommand{\tikzmark}[1]{\tikz[remember picture] \node[coordinate] (#1) {#1};}
% Options:
%  - inst:      Institute
%                 med:      MedFak FAU theme
%                 nat:      NatFak FAU theme
%                 phil:     PhilFak FAU theme
%                 rw:       RWFak FAU theme
%                 rw-jura:  RWFak FB Jura FAU theme
%                 rw-wiso:  RWFak FB WISO FAU theme
%                 tf:       TechFak FAU theme
%  - image:     Cover image on title page
%  - plain:     Plain title page
%  - longtitle: Title page layout for long title
\usetheme[%
  image,%
  longtitle,%
  tf
]{fau}

% Enable semi-transparent animation preview
\setbeamercovered{transparent}


\lstset{%
  language=Python,
  tabsize=2,
  basicstyle=\tt,
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red},
  numbers=left,
  numbersep=0.5em,
  xleftmargin=1em,
  numberstyle=\tt
}


% Title, authors, and date
\title[KDD]{Chapter III: Preprocessing}
\subtitle{Knowledge Discovery in Databases}
\author[L.~Melodia]{Luciano Melodia M.A.}
% English version
\institute[Department]{Evolutionary Data Management, Friedrich-Alexander University Erlangen-NÃ¼rnberg}
% German version
%\institute[Lehrstuhl]{Lehrstuhl, Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg}
\date{Summer semester 2021}
% Set additional logo (overwrites FAU seal)
%\logo{\includegraphics[width=.15\textwidth]{themefau/art/xxx/xxx.pdf}}
\begin{document}
  % Title
  \maketitle

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Chapter III: Preprocessing}
    This is our agenda for this lecture:
        \begin{itemize}
            \item \textbf{Data preprocessing: an overview.}
            \begin{itemize}
              \item Data quality.
              \item Major tasks in data preprocessing.
            \end{itemize}
            \item Data cleaning.
            \item Data integration.
            \item Data reduction.
            \item Data transformation and data discretization.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Quality: Why Preprocess the Data?}
        \begin{itemize}
            \item \textbf{Measures for {\color{airforceblue}data quality}: A multidimensional view:}
            \begin{itemize}
              \item \textbf{Accuracy:} correct or wrong, accurate or not.
              \item \textbf{Completeness:} not recorded, unavailable.
              \item \textbf{Consistency:} some modified but some not, dangling refs, etc.
              \item \textbf{Timeliness:} timely updated?
              \item \textbf{Believability:} how trustworthy is it, that the data is correct?
              \item \textbf{Interpretability:} how easily can the data be understood?
              \item And even many more!
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Major Tasks in Data Preprocessing}
      \begin{itemize}
        \item \textbf{Data cleaning:}
        \begin{itemize}
          \item Fill in missing values.
          \item Smooth noisy data.
          \item Identify or remove outliers.
          \item Resolve inconsistencies.
        \end{itemize}
        \item \textbf{Data integration:}
        \begin{itemize}
          \item Integration of multiple databases.
          \item Data cubes or files.
        \end{itemize}
        \item \textbf{Data reduction:}
        \begin{itemize}
          \item Dimensionality reduction.
          \item Numerosity reduction.
          \item Data compression.
        \end{itemize}
        \item \textbf{Data transformation and data discretization:}
        \begin{itemize}
          \item Normalization.
          \item Concept-hierarchy generation.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Chapter III: Preprocessing}
        \begin{itemize}
            \item Data preprocessing: an overview.
            \begin{itemize}
              \item Data quality.
              \item Major tasks in data preprocessing.
            \end{itemize}
            \item \textbf{Data cleaning.}
            \item Data integration.
            \item Data reduction.
            \item Data transformation and data discretization.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Cleaning}
      \textbf{Data in the real world is {\color{airforceblue}dirty}. Lots of potentially incorrect data:}
      \begin{itemize}
        \item E.g. instrument faulty, human or computer error, transmission error.
        \item \textbf{\color{airforceblue}Incomplete:} lacking attributes, lacking certain attributes of interest or containing aggregate data.
        \begin{itemize}
          \item E.g. occupation = "" (missing data).
        \end{itemize}
        \item \textbf{\color{airforceblue}Noisy:} containing noise, errors or outliers.
        \begin{itemize}
          \item Stochastic deviation, imprecision.
          \item E.g. measurements.
        \end{itemize}
        \item \textbf{\color{airforceblue}Inconsistencies:} containing discrepancies in codes or names.
        \begin{itemize}
          \item E.g. age = "42", birthday = "03/07/2010".
          \item Was rating "1,2,3" and now it is "A,B,C".
          \item Discrepancy between duplicate records (e.g. address old and new).
        \end{itemize}
        \item \textbf{\color{airforceblue}Intentional} (only default value, e.g. disguised missing data):
        \begin{itemize}
          \item Jan. 1 as everyone's birthday?
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Incomplete (Missing) Data}
    \begin{itemize}
      \item \textbf{Data is not always available.}
      \begin{itemize}
        \item E.g. many tuples have no recorded value for several attributes.
        \item Examples are customer income in sales data.
      \end{itemize}
      \item \textbf{Missing data may be due to:}
      \begin{itemize}
        \item Equipment malfunction.
        \item Inconsistency with other recorded data and thus deleted.
        \item Data not entered due to misunderstanding.
        \item Certain data may not be considered important at the time of entry.
        \item Not registered history or changes of the data.
      \end{itemize}
      \item \textbf{Missing data may need to be inferred.}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{How to Handle Missing Data?}
    \begin{itemize}
      \item \textbf{Ignore the tuple:}
      \begin{itemize}
        \item Usually done when class label is missing (when doing classification).
        \item Not effective when the percentage of missing values per attribute varies considerably.
      \end{itemize}
      \item \textbf{Fill in the missing value manually.}
      \begin{itemize}
        \item Tedious or infeasible.
      \end{itemize}
      \item \textbf{Fill in automatically with:}
      \begin{itemize}
        \item A global constant, e.g. "unkown", maybe a new class.
        \item The attribute mean.
        \item The attribute mean for all samples belonging to the same class.
        \item \textbf{\color{airforceblue} The most probable value:} Inference-based such as Bayesian formula or decision tree.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Noisy Data}
    \begin{itemize}
      \item \textbf{\color{airforceblue}Noise:}
      \begin{itemize}
        \item Random error or variance in a measured variable.
        \item Stored value a little bit off the real value, up or down.
        \item Leads to (slightly) incorrect attribute values.
      \end{itemize}
      \item \textbf{May be due to:}
      \begin{itemize}
        \item Faulty or imprecise data-collection instruments.
        \item Data-entry problems.
        \item Data-transmission problems.
        \item Technology limitation.
        \item Inconsistency in naming conventions.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{How to Handle Noisy Data?}
    \begin{itemize}
        \item \textbf{Binning:}
        \begin{itemize}
          \item First sort data and partition into (equal-frequency) bins.
          \item Then smooth by bin mean, by bin median or by bin boundaries.
        \end{itemize}
        \item \textbf{Regression:}
        \begin{itemize}
          \item Smooth by fitting the data to regression functions.
        \end{itemize}
        \item \textbf{Clustering:}
        \begin{itemize}
          \item Detect and remove outliers.
        \end{itemize}
        \item \textbf{Combined computer and human inspection:}
        \begin{itemize}
          \item Detect suspicious values and check by human.
          \item E.g. deal with possible outliers.
        \end{itemize}
    \end{itemize}
    \end{frame}
  }

 {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Cleaning as a Process}
    \begin{itemize}
      \item \textbf{Data-discrepancy detection:}
      \begin{itemize}
        \item Use \textbf{\color{airforceblue}metadata} (e.g. domain, range, dependency, distribution).
        \item Check field overloading.
        \item Check uniqueness rule, consecutive rule and null rule.
        \item Use commercial tools:
        \begin{itemize}
          \item \textbf{\color{airforceblue}Data scrubbing:} use simple domain knowledge (e.g. postal code, spell-check) to detect errors and make corrections.
          \item \textbf{\color{airforceblue}Data auditing:} by analyzing data to discover rules and relationsships to detect violators (e.g. correlation and clustering to find outliers).
        \end{itemize}
        \item \textbf{Data migration and integration:}
        \begin{itemize}
          \item Data-migration tools: allow transformations to be specified.
          \item ETL (Extraction/Transformation/Loading) tools: allow users to specify transformations through a graphical user interface.
        \end{itemize}
        \item \textbf{Integration of the two processes.}
        \begin{itemize}
          \item Iterative and interactive (e.g. the Potter's Wheel tool).
        \end{itemize}
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Chapter III: Preprocessing}
        \begin{itemize}
            \item Data preprocessing: an overview.
            \begin{itemize}
              \item Data quality.
              \item Major tasks in data preprocessing.
            \end{itemize}
            \item Data cleaning.
            \item \textbf{Data integration.}
            \item Data reduction.
            \item Data transformation and data discretization.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Integration}
    \begin{itemize}
       \item \textbf{Data integration:}
       \begin{itemize}
        \item Combine data from multiple sources into a coherent store.
       \end{itemize}
       \item \textbf{Schema integration:}
       \begin{itemize}
        \item E.g. \texttt{A.cust-id} $\equiv$ \texttt{B.cust-\#}.
        \item Integrate metadata from different sources.
       \end{itemize}
       \item \textbf{Entity-identification problem:}
       \begin{itemize}
        \item Identify the same real-world entities from multiple data sources.
        \item E.g. Bill Clinton = William Clinton.
       \end{itemize}
       \item \textbf{Detecting and resolving {\color{airforceblue}data-value conflicts}:}
       \begin{itemize}
        \item For the same real world entity, attribute values from different sources are different.
        \item Possible reasons:
        \begin{itemize}
          \item Different representations (coding).
          \item Different scales, e.g. metric vs. British units.
        \end{itemize}
       \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Handling Redundancy in Data Integration}
    \begin{itemize}
      \item \textbf{Redundant data often occur when integrating multiple databases.}
      \begin{itemize}
        \item \textbf{Object (entity) identification:} \\
              The same attribute or object may have different names in different databases.
        \item \textbf{Derivable data:}\\
              One attribute may be a "derived" attribute in another table. E.g. annual revenue.
      \end{itemize}
      \item \textbf{Redundant attributes:}
      \begin{itemize}
        \item Can be detected by \textbf{\color{airforceblue}correlation analysis} and \textbf{\color{airforceblue}covariance analysis}.
      \end{itemize}
      \item \textbf{Careful integration of the data from multiple sources:}
      \begin{itemize}
        \item Helps to reduce/avoid redundancies and inconsistencies and improve mining speed and quality.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Correlation Analysis for Nominal Data (I)}
      \begin{itemize}
        \item \textbf{Two categories:}
        \begin{itemize}
          \item $A$ has $n$ distinct values: $A := \{a_1, a_2, \ldots, a_n\}$.
          \item $B$ has $m$ distinct values: $B := \{b_1, b_2, \ldots, b_m\}$.
        \end{itemize}
        \item \textbf{\color{airforceblue}Contingency table:}
        \begin{itemize}
          \item Given a multiset of paired data points: $X = \{(a_i,b_j) \; \vert \; a_i \in A \; \text{and} \; b_j \in B\}$.
          \item Columns: the $a_i$ values of $A$.
          \item Rows: the $b_j$ values of $B$.
          \item Cells: counts of data points with \\
          $c_{ij} = \#(\{(a_i,b_j) \in X \})$.
        \end{itemize}
        \item \textbf{Expected count in cell $c_{ij}$:}
        \begin{align}
          e_{ij} = \frac{\sum_{k=1}^{m} c_{ik}\sum_{l=1}^{n} c_{lj}}{\#(X)}
        \end{align}
        \item where $\#(X)$ is the total count of data points.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Correlation Analysis for Nominal Data (II)}
      \begin{itemize}
        \item \textbf{\color{airforceblue}$\chi^2$-test:}
        \begin{align}
          \chi^2 = \sum_{i=1}^{N}\sum_{j=1}^{M} \frac{(c_{ij}-e_{ij})^2}{e_{ij}}.
        \end{align}
        \item Summing over all cells of the contingency table.
        \item No correlation (i.e. independence of attributes) yields $\chi^2$ value of zero.
        \item The larger the $\chi^2$ value, the more likely the variables are related.
        \item The cells that contribute the most to the $\chi^2$ value
are those whose actual count is very different from the expected count $e_{ij}$.
      \end{itemize}
      \begin{itemize}
        \item \textbf{Correlation does not imply causality!}
        \item E.g. $\#$ of hospitals and $\#$ of car-thefts in a city are correlated.
        \item Both are causally linked to the third variable: population.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{$\chi^2$ Calculation: An Example}
    \centering
    \begin{tabular}{l|c|c|c|}
    & Play chess & Not play chess & Sum (row)\\\hline
    Like Science fiction & $250 (90)$ & $200 (360)$ & $450$ \\\hline
    Not like science fiction & $50 (210)$ & $1000 (840)$ & $1050$\\\hline
    Sum (column) & $300$ & $1200$ & $1500$\\\hline
    \end{tabular}
    \begin{itemize}
      \item Numbers in parenthesis are expected counts calculated based on the data distribution in the two categories.
      \item $\chi^2$ calculation:
      \begin{align}
      \chi^2 = \frac{(250-90)^2}{90} + \frac{(50-210)^2}{210} + \frac{(200-360)^2}{360} + \frac{(1000-840)^2}{840} = 507.93.
      \end{align}
      \item Degrees of freedom are $(n-1)\cdot(m-1) = (2-1)(2-1) = 1$.
      \item The $\chi^2$ value for a significance level of $0.001$ is $10.828$.
      \item It shows that "like science fiction" and "play chess" are correlated in the group.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Correlation Analysis of Numerical Data}
    \begin{itemize}
      \item \textbf{\color{airforceblue}Correlation coefficient:}
      \begin{itemize}
        \item Also called Pearson's product-moment coefficient given a paired multiset of data points  $\{(a_i,b_i) \; \vert \; a_i \in A \; \text{and} \; b_i \in B\}$:
        \begin{align}
          r_{A,B} = \frac{\sum_{i=1}^{N} (a_i-\mu_A)(b_i-\mu_B)}{N\sigma_A\sigma_B} = \frac{\sum_{i=1}^{N}(a_ib_i)-N\mu_A\mu_B}{N \sigma_A\sigma_B}.
        \end{align}
        where $N = \#A = \#B$, $\mu_A$ and $\mu_B$ are the means of $A$ and $B$, respectively. $\sigma_A$ and $\sigma_B$ denote the corresponding standard deviations.
      \end{itemize}
      \item If $r_{A,B} > 0$, $A$ and $B$ are positively correlated ($A$'s values increase with $B$'s). \\
      The higher, the stronger the correlation.
      \item $r_{A,B} = 0$: independent.
      \item $r_{A,B} < 0$: negatively correlated.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Visually Evaluating Correlation}
    \begin{figure}[H]
        \centering
        \begin{minipage}{0.32\textwidth}
            \centering
            \begin{tikzpicture}
            \draw[->, thick] (-1.7,0)--(1.7,0) node[right]{$x$};
            \draw[->, thick] (0,-1.7)--(0,1.7) node[above]{$y$};
            \foreach \x in {-1.7,-1.5,...,1.7}{
                    \pgfmathsetmacro\xcoord{\x+rand/10}
                    \pgfmathsetmacro\ycoord{\x+rand/2}
                    \pgfmathsetmacro\xcoord{\xcoord < -1.7 ? -1.7 : \xcoord}
                    \pgfmathsetmacro\xcoord{\xcoord > 1.7 ? 1.7 : \xcoord}
                    \pgfmathsetmacro\ycoord{\ycoord < -1.7 ? -1.7 : \ycoord}
                    \pgfmathsetmacro\ycoord{\ycoord > 1.7 ? 1.7 : \ycoord}
                    \node[circle,draw,fill=black,scale=0.3] at (\xcoord,\ycoord) {};
                }
            \end{tikzpicture}
            \caption{a) Positive correlation.}
        \end{minipage}\hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \begin{tikzpicture}
            \draw[->, thick] (-1.7,0)--(1.7,0) node[right]{$x$};
            \draw[->, thick] (0,-1.7)--(0,1.7) node[above]{$y$};
            \foreach \x in {-1.7,-1.5,...,1.7}{
                    \pgfmathsetmacro\xcoord{\x+rand/10}
                    \pgfmathsetmacro\ycoord{rand*2}
                    \pgfmathsetmacro\xcoord{\xcoord < -1.7 ? -1.7 : \xcoord}
                    \pgfmathsetmacro\xcoord{\xcoord > 1.7 ? 1.7 : \xcoord}
                    \pgfmathsetmacro\ycoord{\ycoord < -1.7 ? -1.7 : \ycoord}
                    \pgfmathsetmacro\ycoord{\ycoord > 1.7 ? 1.7 : \ycoord}
                    \node[circle,draw,fill=black,scale=0.3] at (\xcoord,\ycoord) {};
                }
            \end{tikzpicture}
            \caption{b) Uncorrelated/no correlation.}
        \end{minipage}\hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \begin{tikzpicture}
            \draw[->, thick] (-1.7,0)--(1.7,0) node[right]{$x$};
            \draw[->, thick] (0,-1.7)--(0,1.7) node[above]{$y$};
            \foreach \x in {-1.7,-1.5,...,1.7}{
                    \pgfmathsetmacro\xcoord{\x+rand/10}
                    \pgfmathsetmacro\ycoord{-\x+rand/2}
                    \pgfmathsetmacro\xcoord{\xcoord < -1.7 ? -1.7 : \xcoord}
                    \pgfmathsetmacro\xcoord{\xcoord > 1.7 ? 1.7 : \xcoord}
                    \pgfmathsetmacro\ycoord{\ycoord < -1.7 ? -1.7 : \ycoord}
                    \pgfmathsetmacro\ycoord{\ycoord > 1.7 ? 1.7 : \ycoord}
                    \node[circle,draw,fill=black,scale=0.3] at (\xcoord,\ycoord) {};
                }
            \end{tikzpicture}
            \caption{c) Negative correlation.}
        \end{minipage}\hfill
    \end{figure}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Covariance of Numerical Data (I)}
    \begin{itemize}
      \item \textbf{\color{airforceblue}Covariance} \textbf{is similar to correlation:}\\
            \begin{align}
              \text{Cov}(A,B) = \frac{\sum_{i=1}^{n}(a_i-\overline{A})(b_i-\overline{B})}{N}
            \end{align}
      \item \textbf{Pearson's correlation coefficient:}\\
            \begin{align}
              r_{A,B} = \frac{\sum_{i=1}^{N} (a_i-\mu_A)(b_i-\mu_B)}{N\sigma_A\sigma_B} = \frac{\sum_{i=1}^{N}(a_ib_i)-N\mu_A\mu_B}{N \sigma_A\sigma_B},
            \end{align}
            where $N$ is the number of tuples.
    \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Covariance of Numerical Data (II)}
    \begin{itemize}
      \item \textbf{Positive covariance:}\\
            If $\text{Cov}(A,B) > 0$, then $A$ and $B$ tend to be either both larger or both smaller than their expected values.
      \item \textbf{Negative covariance:}\\
            If $\text{Cov}(A,B) < 0$, then if $A$ is larger than its expected value, $B$ is likely to be smaller than its expected value and vice versa.
      \item \textbf{Independence:}
      \begin{itemize}
        \item $\text{Cov}(A,B) = 0$.
        \item \textbf{\color{airforceblue}But the converse is not true:} Some pairs of random variables may have a covariance of $0$ but are not independent. Only under some additional assumptions (e.g., the data follow multivariate normal distributions) does a covariance of $0$ imply independence.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Covariance: An Example (I)}
    \begin{itemize}
      \item \textbf{Can be simplified in computation as:}
            \begin{align}
              \text{Cov}(A,B) &= E((A - E(A))(B-E(B)))\\
                              &= E(AB-AE(B)-E(A)B+E(A)E(B))\\
                              &= E(AB)-E(A)E(B)-E(A)E(B)+E(A)E(B)\\
                              &= E(AB)-E(A)E(B).
            \end{align}
    \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Covariance: An Example (II)}
    \begin{itemize}
      \item Suppose two stocks $A$ and $B$ have the following values within some time:\\
      $(2,5), (3,8), (5,10), (4,11), (6,14).$
      \item If the stocks are affected by the same industry trends, will their prices rise or fall together?
      \begin{align}
        E(A) &= \frac{2+3+5+4+6}{5} = \frac{20}{5} = 4.\\
        E(B) &= \frac{5+8+10+11+14}{5} = \frac{48}{5} = 9.6.\\
        \text{Cov}(A,B) &= \frac{2\cdot5 + 3\cdot 8 + 5 \cdot 10 + 4 \cdot 11 + 6 \cdot 14}{5} - 4\cdot 9.6 = 4.
      \end{align}
      \item Thus, $A$ and $B$ rise together since $\text{Cov}(A,B) > 0$.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Chapter III: Preprocessing}
        \begin{itemize}
            \item Data preprocessing: an overview.
            \begin{itemize}
              \item Data quality.
              \item Major tasks in data preprocessing.
            \end{itemize}
            \item Data cleaning.
            \item Data integration.
            \item \textbf{Data reduction.}
            \item Data transformation and data discretization.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Reduction (I): Dimensionality Reduction}
        \begin{itemize}
            \item \textbf{Curse of dimensionality:}
            \begin{itemize}
              \item When dimensionality increases data becomes increasingly sparse.
              \item Density and distance between points, which are critical to clustering and outlier analysis become less meaningful.
              \item The possible combinations of subspaces will grow exponentially.
            \end{itemize}
            \item \textbf{Dimensionality reduction:}
            \begin{itemize}
              \item Avoid the curse of dimensionality.
              \item Help eliminate irrelevant features and reduce noise.
              \item Reduce time and space required in data mining.
              \item Allow easier visualization.
            \end{itemize}
            \item \textbf{Dimensionality-reduction techniques:}
            \begin{itemize}
              \item Wavelet transforms.
              \item Principal component analysis.
              \item Supervised and nonlinear techniques (e.g. feature selection).
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Reduction Strategies}
        \begin{itemize}
            \item Obtain a reduced representation of the data set that is much smaller in volume but yet produces the same (or almost the same)  results.
            \item \textbf{Why data reduction?}\\
            \begin{itemize}
              \item A database/data warehouse may store terabytes of data.
              \item Complex data analysis may take a very long time to run on the complete data set.
            \end{itemize}
            \item \textbf{Data reduction strategies:}
            \begin{itemize}
              \item Dimensionality reduction, i.e. remove unimportant attributes.
              \begin{itemize}
                \item Wavelet transforms.
                \item Principal component analysis.
                \item Attribute subset selection or attribute creation.
              \end{itemize}
              \item Numerosity reduction:
              \begin{itemize}
                \item Regression and log-linear models.
                \item Histograms, clustering and sampling.
                \item Data cube aggregation.
              \end{itemize}
              \item Data compression.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Mapping Data to a New Space}
    \begin{itemize}
      \item \textbf{Fourier transform}.
      \item \textbf{Wavelet transform.}

    \begin{figure}
      \centering
      \begin{minipage}[b]{0.30\textwidth}
        \includegraphics[width=5cm]{img/twosinewaves.png}
        \caption{Two sine waves.}
      \end{minipage}\hfill
      \begin{minipage}[b]{0.30\textwidth}
        \includegraphics[width=5cm]{img/twosinewaveswithnoise.png}
        \caption{Two sine waves with noise.}
      \end{minipage}\hfill
      \begin{minipage}[b]{0.30\textwidth}
        \includegraphics[width=5cm]{img/frequencies.png}
        \caption{Frequencies.}
      \end{minipage}
    \end{figure}
    \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{What is Wavelet Transform?}
    \begin{minipage}[b]{0.55\textwidth}
      \begin{itemize}
        \item \textbf{Decomposes a signal into different frequency subbands.}\\
              Applicable to $n$-dimensional signals.
        \item Data transformed to preserve relative distance between objects at different levels of resolution.
        \item Allow natural clusters to become more distinguishable.
        \item Used for image compression.
      \end{itemize}
    \end{minipage}\hspace{1cm}
    \begin{minipage}[b]{0.30\textwidth}
      \includegraphics[width=5cm]{img/wavelettransform.png}
    \end{minipage}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Wavelet Transform}
    \begin{itemize}
      \item \textbf{Discrete wavelet transform:}\\
            Transforms a vector $X$ into a different vector $X'$ of wavelet coefficients with the same length.
      \item \textbf{Compressed approximation:}\\
            Store only a small fraction of the strongest of the wavelet coefficients.
      \item \textbf{Similar to discrete fourier transform, but better lossy compression, localized in space.}
      \item \textbf{Method:}
      \begin{itemize}
        \item The length of the vector must be an integer power of $2$ (padding with $0$'s if necessary).
        \item Each transform has two functions: smoothing and difference.
        \item Applied to pairs of data, resulting in two sets of data with half the length.
        \item The two functions are applied recursively until reaching the desired length.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Wavelet Decomposition}
    \begin{itemize}
      \item \textbf{Example:}
      \begin{align}
        X &= (2,2,0,2,3,5,4,4), \text{can be transformed to}\\
        X' &= (2.75,-1.25,0.5,0,0,-1,-1,0).
      \end{align}
      \item \textbf{Compression:}\\
            Many small detail coefficients can be replaced by $0$'s, \\
            and only the significant coefficients are retained.
    \end{itemize}
    \vspace{0.2cm}
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      \text{Resolution} & \text{Averages} & \text{Detail coefficients}\\\hline
      $8$ & $(2,2,0,2,3,5,4,4)$ & - \\\hline
      $4$ & $(2,1,4,4)$ & $(0,-1,-1,0)$ \\\hline
      $2$ & $(1 \frac{1}{2},4)$ & $(\frac{1}{2},0)$ \\\hline
      $1$ & $(2 \frac{3}{4})$ & $(-1 \frac{1}{4})$\\\hline
    \end{tabular}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Why Wavelet Transform?}
    \begin{itemize}
      \item \textbf{Use hat-shaped filters:}
      \begin{itemize}
        \item Emphasize region where points cluster.
        \item Suppress weaker information in their boundaries.
      \end{itemize}
      \item \textbf{Effective removal of outliers:}
      \begin{itemize}
        \item Insensitive to noise, insensitive to input order.
      \end{itemize}
      \item \textbf{Multi-resolution:}
      \begin{itemize}
        \item Detect arbitrary shaped clusters at different scales.
      \end{itemize}
      \item \textbf{Efficient:} Complexity $\mathcal{O}(N)$.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis (I)}
    \begin{itemize}
      \item Principal component analysis is a method of summarizing \\
      the properties of a set of multivariate data samples.
      \item It is a \textbf{linear transformation} method that is often used \\ for data analysis or data compression.
      \item Principal component analysis is often also called \textbf{Karhunen-Loeve transformation}.
      \item PCA is equivalent to maximization of the information \\ at the output of a neural network with linear neurons.
    \end{itemize}
    \vspace{0.5cm}
    The goal of the principal component analysis (PCA) is the identification \\
    of $m$ \textbf{normed orthogonal vectors} $\{x_i \in \mathbb{R}^n \; \vert \; i = 1,2, \ldots,m \}$ \\
    within the input space, which \textbf{represent most of the variance} of the data.
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: The Problem (II)}
    \begin{itemize}
      \item Consider a set of data points $\{\mathbf{x}_i = (x_{i1},x_{i2},\ldots,x_{in}) \; \vert \; \mathbf{x}_i \in \mathbb{R}^n \}$, where each dimension measures some physical quantity.
      \item Consider an orthonormal system of $n$-vectors with dimension $n$: $\{\mathbf{b}_i = (b_{i1},b_{i2},\ldots,b_{in}) \; \vert \; \mathbf{b}_i \in \mathbb{R}^n \}$.
      \begin{itemize}
        \item The \textbf{orthonormal system} forms a basis of the vector space, where we have recorded our data. Thus our data can be expressed as a linear combination of $\{\mathbf{b}_i\}$.
      \end{itemize}
      \begin{align}
        \mathbf{B} = \begin{bmatrix}
          \mathbf{b}_1 \\
          \mathbf{b}_2 \\
          \vdots \\
          \mathbf{b}_n
        \end{bmatrix} = \begin{bmatrix}
          1 & 0 & \cdots & 0 \\
          0 & 1 & \cdots & 0 \\
          \vdots & \vdots & \ddots & \vdots \\
          0 & 0 & \cdots & 1
        \end{bmatrix}
      \end{align}
      \item Each row is an orthonormal basis vector $\mathbf{b}_i$ with $n$-components. Thus we have a $n \times n$-matrix.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: The Problem (III)}
    \begin{itemize}
      \item \textbf{Is there another basis, which is a linear combination of the initially chosen basis, \\
      that best re-expresses our data set?}
      \item Let $\mathbf{X}$ we the data matrix with $n \times m$ records.
      \item Let $\mathbf{Y}$ be another $n \times m$-matrix, related by some linear transformation $\mathbf{U}$, such that
      \begin{align}
         \mathbf{U}\mathbf{X} &= \mathbf{Y},\\
         \begin{bmatrix}
           \mathbf{u}_1 \\
           \mathbf{u}_2 \\
           \vdots \\
           \mathbf{u}_n
         \end{bmatrix}
         \begin{bmatrix}
           \mathbf{x}_1 & \mathbf{x}_2 & \cdots & \mathbf{x}_m
         \end{bmatrix} &=
         \begin{bmatrix}
           \mathbf{u}_1\mathbf{x}_1 & \mathbf{u}_1\mathbf{x}_2 & \cdots & \mathbf{u}_1\mathbf{x}_m \\
           \mathbf{u}_2\mathbf{x}_1 & \mathbf{u}_2\mathbf{x}_2 & \cdots & \mathbf{u}_2\mathbf{x}_m \\
           \vdots & \vdots & \ddots & \vdots \\
           \mathbf{u}_n\mathbf{x}_1 & \mathbf{u}_n\mathbf{x}_2 & \cdots & \mathbf{u}_n\mathbf{x}_m
         \end{bmatrix}.
      \end{align}
      \item Eq. 17 and 18 represent the \textbf{change of a basis} and can be interpreted as:
      \begin{itemize}
        \item[1.] $\mathbf{U}$ is a matrix that transforms $\mathbf{X}$ into $\mathbf{Y}$.
        \item[2.] Geometrically, $\mathbf{U}$ is a rotation and stretch which again transforms $\mathbf{X}$ into $\mathbf{Y}$.
        \item[3.] The rows of $\mathbf{U}$, $\{\mathbf{u}_1,\ldots,\mathbf{u}_n\}$, are a set of new basis vectors for expressing the columns of $\mathbf{X}$.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: The Problem (IV)}
    \begin{itemize}
      \item \textbf{Questions we have to answer:}
      \begin{itemize}
        \item What is the best way to "re-express" $\mathbf{X}$?
        \item What is a good choice of basis $\mathbf{U}$?
      \end{itemize}
      \item \textbf{Main assumptions:}
      \begin{itemize}
        \item Continuity of data space.
        \item Linearity of the basis vectors.
        \item Mean and variance are sufficient statistics.
        \item Large variance have important dynamics in data.
        \item Principal components are orthogonal.
      \end{itemize}
      \item PCA assumes that the basis vectors are orthonormal, i.e., $\mathbf{u}_i\cdot\mathbf{u}_j = \delta_{ij}$, \\
      such that $\mathbf{U}$ is an orthonormal matrix.
      \item PCA assumes that the directions with the largest variances are most \emph{principal}.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Solving the Problem}
    \begin{itemize}
      \item Find some orthonormal matrix $\mathbf{U}$ where $\mathbf{Y}=\mathbf{U}\mathbf{X}$ such that $\mathbf{Cov}(\mathbf{Y},\mathbf{Y}) \equiv \frac{1}{n-1} \mathbf{Y}\mathbf{Y}^T$ is diagonalized. The rows of $\mathbf{U}$ are the \emph{principal components} of $\mathbf{X}$.
      \item Recall, high diagonal values of $\mathbf{Cov}(\mathbf{Y},\mathbf{Y})$ mean a covariance matrix with large variance, thus interesting dynamics. Large off-diagonal values mean high covariance, thus high redundancy.
      \item We rewrite $\mathbf{Cov}(\mathbf{Y},\mathbf{Y})$ in terms of $\mathbf{U}$:
      \begin{align}
        \mathbf{Cov}(\mathbf{Y},\mathbf{Y}) &= \frac{1}{n-1} \mathbf{Y}\mathbf{Y}^T \\
        &= \frac{1}{n-1} (\mathbf{U}\mathbf{X})(\mathbf{U}\mathbf{X})^T \\
        &= \frac{1}{n-1} \mathbf{U}\mathbf{X}\mathbf{X}^T\mathbf{U}^T \\
        &= \frac{1}{n-1} \mathbf{U}\mathbf{A}\mathbf{U}^T.
      \end{align}
      \item We have a new matrix $\mathbf{A} \equiv \mathbf{X}\mathbf{X}^T$, which is \textbf{symmetric}.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Diagonalization of Symmetric Matrices (I)}
    \begin{itemize}
      \item The symmetric matrix $\mathbf{A}$ can be diagonalized by an orthogonal matrix of its eigenvectors.
      \item Orthogonally diagonalizable means that there exists some $\mathbf{E}$, such that $\mathbf{A} = \mathbf{E}\mathbf{D}\mathbf{E}^T$, where $\mathbf{D}$ is a diagonal matrix and $\mathbf{E}$ is a matrix diagonalizing $\mathbf{A}$. Let's compute $\mathbf{A}^T$:
      \begin{align}
        \mathbf{A}^T = (\mathbf{E}\mathbf{D}\mathbf{E}^T)^T = \mathbf{E}^{TT}\mathbf{D}^T\mathbf{E}^T = \mathbf{EDE}^T = \mathbf{A}.
      \end{align}
      Thus, if $\mathbf{A}$ is orthogonally diagonalizable, it must also be symmetric.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Diagonalization of Symmetric Matrices (II)}
    \begin{itemize}
      \item Let $\mathbf{E} = [\mathbf{e}_1 \; \mathbf{e}_2 \; \cdots \; \mathbf{e}_m]$ be a matrix of normalized eigenvectors $\mathbf{e}_i$. We briefly cover, that a matrix can only be orthogonally diagonalized iff that matrix's eigenvectors are linearly independent. Further, we show that such a matrix has not only linearly independent, but orthogonal eigenvectors.
      \item Let $\mathbf{A}$ be a matrix, not necessarily symmetric, with linearly independent eigenvectors. Let further $\mathbf{D}$ be a diagonal matrix where the $i$th eigenvalue is placed in $\mathbf{D}_{ii}$.
      \item We show that $\mathbf{AE} = \mathbf{ED}$.
      \begin{align}
        \text{Left hand side:} \quad \mathbf{AE} = [\mathbf{Ae}_1 \; \mathbf{Ae}_2 \; \cdots \; \mathbf{Ae}_m], \\
        \text{Right hand side:} \quad \mathbf{ED} = [\lambda\mathbf{e}_1 \; \lambda\mathbf{e}_2 \; \cdots \; \lambda\mathbf{e}_m].
      \end{align}
      \item Note, that if $\mathbf{AE} = \mathbf{ED}$ then $\mathbf{Ae}_i = \lambda_i\mathbf{e}_i$ for all $i$. \\
      \item This is the definition of the eigenvalue equation, thus $\mathbf{AE} = \mathbf{ED}$ follows.
      \item $\implies$ $\mathbf{A} = \mathbf{EDE}^{-1} = \mathbf{EDE}^{T}$.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Intermezzo: Inverse of Orthogonal Matrix is its Transposed}
    \begin{itemize}
      \item Let $\mathbf{E}$ be an orthogonal matrix, then $\mathbf{E}^{-1} = \mathbf{E}^T$.
      \item Write $\mathbf{E}$ as $\mathbf{E} = [\mathbf{e}_1 \; \mathbf{e}_2 \; \cdots \; \mathbf{e}_n]$, where $\mathbf{e}_i$ is the $i$th column vector. We now have to show that $\mathbf{E}^T\mathbf{E} = \mathbf{I}$, where $\mathbf{I}$ is the identity matrix.
      \item Note that $(\mathbf{E}^T\mathbf{E})_{ij} = \mathbf{e}_i^T\mathbf{e}_j$. As $\mathbf{E}$ is orthogonal, the dot product is $0$ for any two column vectors. \item The only exception is the dot product of one particular column with itself, \\
      which equals one:
      \begin{align}
        (\mathbf{E}^T\mathbf{E})_{ij} = \mathbf{e}_i^T\mathbf{e}_j =
        \begin{cases}
          1 & i=j, \\
          0 & i \neq j.
        \end{cases}
      \end{align}
      \item $\mathbf{E}^T\mathbf{E} = \mathbf{I}$ and by definition we have that $\mathbf{E}^{-1}\mathbf{E} = \mathbf{I}$.
      \item Therefore, we conclude that $\mathbf{E}^{-1} = \mathbf{E}^T$.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Diagonalization of Symmetric Matrices (III)}
    \begin{itemize}
      \item Now we show that a symmetric matrix has orthogonal eigenvectors.
      \item Let $\lambda_1$ and $\lambda_2$ be distinct eigenvalues for eigenvectors $\mathbf{e}_1$ and $\mathbf{e}_2$, such that
      \begin{align}
        \lambda_1\mathbf{e}_1 \cdot \mathbf{e}_2 &= (\lambda_1 \mathbf{e}_1)^{T} \mathbf{e}_2 \\
        &= (\mathbf{Ae}_1)^{T}\mathbf{e}_2 \\
        &= \mathbf{e}_1^T \mathbf{A}^T \mathbf{e}_2 \\
        &= \mathbf{e}_1^T \mathbf{A}\mathbf{e}_2 \\
        &= \mathbf{e}_1^T (\lambda_2 \mathbf{e}_2) \\
        \lambda_1\mathbf{e}_1 \cdot \mathbf{e}_2 &= \lambda_2 \mathbf{e}_1 \cdot \mathbf{e}_2.
      \end{align}
      \item By the last relation we can equate that $(\lambda_1 - \lambda_2) \mathbf{e}_1 \cdot \mathbf{e}_2 = 0$.
      \item By conjecture, the eigenvalues are unique, thus it holds that $\mathbf{e}_1 \cdot \mathbf{e}_2 = 0$.
      \item We conclude that the eigenvectors of a symmetric matrix are orthogonal.
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Diagonalization of Symmetric Matrices (IV)}
    \begin{itemize}
      \item We defined $\mathbf{A} \equiv \mathbf{X}\mathbf{X}^T$, where $\mathbf{A}$ is symmetric.
      \item Further, we computed $\textbf{Cov}(\mathbf{Y},\mathbf{Y}) = \frac{1}{n-1} \mathbf{UAU}^T$.
      \item By the intermezzo we know that $\mathbf{A} = \mathbf{EDE}^T$, \\
      for some eigenvector matrix $\mathbf{E}$ of $\mathbf{A}$ and diagonal matrix $\mathbf{D}$.
      \item \textbf{What if $\mathbf{A}$ is degenerated?}
      \begin{itemize}
        \item $\mathbf{A}$ has $r \leq n$ orthonormal eigenvectors, where $r = \text{rank} \; \mathbf{A}$.
        \item If $r < n$, then $\mathbf{A}$ is degenerated.
        \begin{itemize}
          \item All data occupy a subspace of dimension $r \leq n$.
        \end{itemize}
        \item We select $(n-r)$ additional orthonormal vectors to \emph{fill up} the matrix $\mathbf{E}$.
        \begin{itemize}
          \item These additional vectors do not effect the final solution \\
          because the variances associated with these directions are zero.
        \end{itemize}
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: The Trick}
    \begin{itemize}
      \item Select $\mathbf{U}$ to be a matrix where each row $\mathbf{u}_i$ is ein eigenvector of $\mathbf{XX}^T$.
      \item Thus, $\mathbf{U} \equiv \mathbf{E}^T$.
      \item Substituting into the result from our intermezzo we get: $\mathbf{A} = \mathbf{U}^T\mathbf{DU}$. We rewrite the equation for $\mathbf{Cov}(\mathbf{Y},\mathbf{Y})$ as
      \begin{align}
        \mathbf{Cov}(\mathbf{Y},\mathbf{Y}) &= \frac{1}{n-1} \mathbf{UAU}^T \\
        &= \frac{1}{n-1} \mathbf{U}(\mathbf{U}^T\mathbf{DU})\mathbf{U}^T \\
        &= \frac{1}{n-1} (\mathbf{UU}^T)\mathbf{D}(\mathbf{UU}^T) \\
        &= \frac{1}{n-1} (\mathbf{UU}^{-1})\mathbf{D}(\mathbf{UU}^{-1}) \\
        \mathbf{Cov}(\mathbf{Y},\mathbf{Y}) &= \frac{1}{n-1} \mathbf{D}.
      \end{align}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Summary}
    \begin{itemize}
      \item We conclude, that our choice of $\mathbf{U}$ diagonalizes $\mathbf{Cov}(\mathbf{Y},\mathbf{Y})$.
      \item \textbf{The results are summarized as follows:}
      \begin{itemize}
        \item The principal components of $\mathbf{X}$ are the eigenvectors of $\mathbf{XX}^T$; or the rows of $\mathbf{U}$.
        \item The $i$th diagonal value of $\mathbf{Cov}(\mathbf{Y},\mathbf{Y})$ is the variance of $\mathbf{X}$ along $\mathbf{u}_i$.
        \item The computation of PCA involves subtracting the mean of each measurement type.
        \item The computation of PCA involves computing the eigenvectors of $\mathbf{XX}^T$.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Principal Component Analysis: Algorithm}
      \begin{itemize}
        \item \texttt{PCA}(data) \{
        \begin{itemize}
          \item $[\text{N,M}]$ = size(data); \emph{\color{gray}\# (N dimensions, M trials).}
          \item means = \texttt{mean}(data,2);
          \item data = data - \texttt{repmat}(means,1,M);
          \item covariance = 1 / (N-1) $\ast$ data $\ast$ $\text{data}^T$;
          \item $[\text{PC,V}]$ = \texttt{eig}(covariance); \emph{\color{gray}\# PC - each column is a PC. V - Nx1 matrix of variances.}
          \item V = \texttt{diag}(V); \emph{\color{gray}\# Extract diagonal of matrix as vector.}
          \item $[\text{junk, rindices}]$ = \texttt{sort}(-1 $\ast$ V); \emph{\color{gray}\# Sort the variances in decreasing order.}
          \item V  = V(rindices);
          \item PC = PC(:,rindices);
          \item signals = $\text{PC}^T$ $\ast$ data;
          \item \textbf{return} signals; \emph{\color{gray}\# Signals - NxM matrix of projected data.}
        \end{itemize}
        \item \};
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Attribute-subset Selection}
    \begin{itemize}
      \item \textbf{Another way to reduce dimensionality of data.}
      \item\textbf{\color{airforceblue}Redundant attributes:}
      \begin{itemize}
        \item Duplicate much or all of the information contained in other attributes.
        \begin{itemize}
          \item E.g. purchase price of a product and the amount of sales tax paid.
        \end{itemize}
      \end{itemize}
      \item \textbf{\color{airforceblue}Irrelevant attributes:}
      \begin{itemize}
        \item contain no information that is useful for the data-mining task at hand.
        \begin{itemize}
          \item E.g. students' ID is often irrelevant to the task of predicting students' GPA.
        \end{itemize}
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Heuristic Search in Attribute Selection}
    \begin{itemize}
      \item \textbf{There are $2^d$ possible attribute combinations of $d$ attributes.}
      \item\textbf{\color{airforceblue}Typical heuristic attribute-selection methods:}
      \begin{itemize}
        \item Best single attribute under the attribute-independence assumption: \\ choose by significance tests (e.g. t-test, see Chapter 6).
        \item Best step-wise feature selection:
        \begin{itemize}
          \item The best single attribute is picked first.
          \item Then next best attribute condition to the first \ldots
        \end{itemize}
      \end{itemize}
      \item \textbf{\color{airforceblue}Step-wise attribute elimination:}
      \begin{itemize}
        \item Repeatedly eliminate the worst attribute.
      \end{itemize}
      \item Best combined attribute selection and elimination.
      \item Optimal branch and bound:
      \begin{itemize}
        \item Use attribute elimination and backtracking.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Attribute Creation (Feature Generation)}
    \begin{itemize}
      \item \textbf{Create new attributes (features) that can capture the important information in a data set more effectively than the original ones.}
      \item \textbf{Three general methodologies:}
      \begin{itemize}
        \item Attribute extraction.
        \begin{itemize}
          \item Domain-specific.
        \end{itemize}
      \item Mapping data to new space (see: data reduction).
      \begin{itemize}
        \item E.g. Fourier transformation, wavelet transformation, manifold approaches (not covered).
      \end{itemize}
      \item Attribute construction:
      \begin{itemize}
        \item Combining features (see: discriminative frequent patterns in Chapter 5).
        \item Data discretization.
      \end{itemize}
     \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Reduction (II): Numerosity Reduction}
    \begin{itemize}
      \item \textbf{Reduce data volume by choosing alternative, {\color{airforceblue}smaller} forms of data representation.}
      \item \textbf{{\color{airforceblue}Parametric} methods (e.g., regression):}
      \begin{itemize}
        \item Assume the data fits some \textbf{{\color{airforceblue}model}} (e.g. a function).
        \item Estimate model parameters.
        \item Store only the parameters.
        \item Discard the data (except possible outliers):
        \begin{itemize}
          \item Ex. log-linear models obtain value at a point in $m$-dimensional space as the product of appropriate marginal subspaces.
        \end{itemize}
      \end{itemize}
      \item \textbf{{\color{airforceblue}Non-parametric} methods:}
      \begin{itemize}
        \item Do not assume models.
        \item Major families: histograms, clustering, sampling, \ldots
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Parametric Data Reduction: Regression and Log-Linear Models}
    \begin{itemize}
      \item \textbf{Linear regression:}
      \begin{itemize}
        \item Data modeled to fit a \textbf{{\color{airforceblue}straight line}}.
        \item Often uses the \textbf{{\color{airforceblue}least-square method}} to fit the line.
      \end{itemize}
      \item \textbf{Multiple regression:}
      \begin{itemize}
        \item Allows a response random variable $Y$ to be modeled as a linear function of a multidimensional feature vector $(x_1, x_2,\ldots, x_n)$.
      \end{itemize}
      \item \textbf{Log-linear model:}
      \begin{itemize}
        \item Approximates discrete multidimensional probability distributions.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Regression Analysis}
    \begin{itemize}
      \item A collective name for techniques for the modeling and analysis of numerical data consisting \\ of values of a \textbf{{\color{airforceblue}dependent variable}} (also called response variable or measurement) and of one or more \textbf{{\color{airforceblue}independent variables}} (aka. explanatory variables or predictors).
      \item The parameters are estimated so as to give a best fit of the data.
      \item The \textbf{{\color{airforceblue}best fit}} is evaluated by using the \textbf{{\color{airforceblue}least-squares method}}, but other criteria have also been used.
      \item Used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships.
    \end{itemize}
    \hspace{4.5cm}
    \resizebox{6cm}{!}{%
    \pgfmathsetseed{1138} % set the random seed
    \pgfplotstableset{ % Define the equations for x and y
        create on use/x/.style={create col/expr={42+2*\pgfplotstablerow}},
        create on use/y/.style={create col/expr={(0.6*\thisrow{x}+130)+5*rand}}
    }
    % create a new table with 30 rows and columns x and y:
    \pgfplotstablenew[columns={x,y}]{30}\loadedtable
    \centering
    \begin{tikzpicture}
    \begin{axis}[
    xlabel=Weight (kg), % label x axis
    ylabel=Height (cm), % label y axis
    axis lines=left, %set the position of the axes
    xmin=40, xmax=105, % set the min and max values of the x-axis
    ymin=150, ymax=200, % set the min and max values of the y-axis
    clip=false
    ]

    \addplot [only marks] table {\loadedtable};
    \addplot [no markers, thick, red] table [y={create col/linear regression={y=y}}] {\loadedtable} node [anchor=west] {$\pgfmathprintnumber[precision=2, fixed zerofill]{\pgfplotstableregressiona} \cdot \mathrm{Weight} + \pgfmathprintnumber[precision=1]{\pgfplotstableregressionb}$};
    \end{axis}
    \end{tikzpicture}}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Regression Analysis and Log-Linear Models}
    \begin{itemize}
      \item \textbf{Linear regression: $y = wx + b$}.
      \begin{itemize}
        \item Two regression coefficients, $w$ and $b$, \\
              specify the line and are to be estimated by using the data at hand.
        \item Using the least-squares criterion to the known values of $y_1,y_2, \ldots, x_1,x_2,\ldots$
      \end{itemize}
      \item \textbf{Multiple regression: $y = w_1 x_1 + w_2 x_2 + \ldots + w_n x_n + b$}.
      \begin{itemize}
        \item Many nonlinear functions can be transformed into the above.
      \end{itemize}
      \item \textbf{Log-linear models:}
      \begin{itemize}
        \item Approximate discrete multidimensional probability distributions.
        \item Estimate the probability of each point (tuple) in a multi-dimensional space for a set of discretized attributes, based on a smaller subset of dimensional combinations.
        \item Useful also for dimensionality reduction and data smoothing.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Histogram Analysis}
    \begin{itemize}
      \item \textbf{Divide data into buckets and store average (sum) of each bucket.}
      \item \textbf{Partitioning rules:}
      \begin{itemize}
        \item Equal-width: equal bucket range.
        \item Equal-frequency (or equal-depth).
      \end{itemize}
    \end{itemize}
    \centering
    \includegraphics[width=10cm]{img/histogram.png}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Clustering}
    \begin{itemize}
      \item \textbf{Partition data set into clusters based on similarity and store cluster representation (e.g., centroid and diameter) only.}
      \begin{itemize}
        \item Can be very effective if data points are close to each other under a certain norm and choice of space.
        \item Can have hierarchical clustering and be stored in multidimensional index-tree structures.
        \item There are many choices of clustering algorithms.
        \item Cluster analysis will be studied in depth in Chapter 7.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Sampling}
    \begin{itemize}
      \item \textbf{Obtain a small sample $x$ to represent the whole data set $X$.}
      \item \textbf{Allow a mining algorithm to run in complexity \\ that is potentially sub-linear to the size of the data.}
      \item \textbf{Key principle: Choose a {\color{airforceblue}representative} subset of the data.}
      \begin{itemize}
        \item Simple random sampling may have very poor performance in the presence of skew.
        \item Develop adaptive sampling methods, e.g. stratified sampling.
      \end{itemize}
      \item \textbf{Note: Sampling may not reduce database I/Os.}
      \begin{itemize}
        \item One page at a time.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Types of Sampling}
        \begin{itemize}
            \item \textbf{Simple random sampling.}
            \begin{itemize}
              \item There is an equal probability of selecting any particular item.
            \end{itemize}
            \item \textbf{Sampling without repetition.}
            \begin{itemize}
              \item Once an object is selected, it is removed from the population.
            \end{itemize}
            \item \textbf{Sampling with repetition.}
            \begin{itemize}
              \item A selected object is not removed from the population.
            \end{itemize}
            \item \textbf{Stratified sampling:}
            \begin{itemize}
              \item Partition the data set and draw samples from each partition: Proportionally, i.e. approximately the same percentage of the data.
              \item Used in conjunction with skewed data.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data-cube Aggregation}
        \begin{itemize}
            \item \textbf{The lowest level of a data cube (base cuboid).}
            \begin{itemize}
              \item The aggregated data for an \textbf{individual entity of interest}.
              \item E.g. a customer in a phone-calling data warehouse.
              \item Number of calls per hour, day, or week.
            \end{itemize}
            \item \textbf{Multiple levels of aggregation in data cubes.}
            \begin{itemize}
              \item Further reduce the size of data to deal with.
            \end{itemize}
            \item \textbf{Reference appropriate levels.}
            \begin{itemize}
              \item Use the smallest representation which is enough to solve the task.
            \end{itemize}
            \item \textbf{Queries regarding aggregated information should be answered using the data cube, if possible.}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Reduction (III): Data Compression}
        \begin{itemize}
            \item \textbf{String compression.}
            \begin{itemize}
              \item There are extensive theories and well-tuned algorithms.
              \item Typically lossless, but only limited manipulation is possible without expansion.
            \end{itemize}
            \item \textbf{Audio/video compression.}
            \begin{itemize}
              \item Typically lossy compression, with progressive refinement.
              \item Sometimes small fragments of signal can be reconstructed without reconstructing the whole.
            \end{itemize}
            \item \textbf{Time sequence is not audio.}
            \begin{itemize}
              \item Typically short and varies slowly with time.
            \end{itemize}
            \item \textbf{Dimensionality and numerosity reduction may also be considered as forms of data compression.}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Chapter III: Preprocessing}
        \begin{itemize}
            \item Data preprocessing: an overview.
            \begin{itemize}
              \item Data quality.
              \item Major tasks in data preprocessing.
            \end{itemize}
            \item Data cleaning.
            \item Data integration.
            \item Data reduction.
            \item \textbf{Data transformation and data discretization.}
            \item Summary.
        \end{itemize}
    \end{frame}
  }

 {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data Transformations}
    \begin{itemize}
      \item Functions applied to a finite set of samples.
      \item \textbf{Methods:}
      \begin{itemize}
        \item Smoothing: Remove noise from data.
        \item Attribute/feature construction: New attributes constructed from the given ones.
        \item Aggregation: Summarization, data-cube construction.
        \item Normalization: Scaled to fall within a smaller, specified range.
        \begin{itemize}
          \item Min-max normalization
          \item Z-score normalization.
          \item Normalization by decimal scaling.
        \end{itemize}
        \item Discretization: concept-hierarchy climbing.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Normalization}
      \begin{itemize}
        \item \textbf{Min-max normalization (to some interval $[\text{min},\text{max}]$):}
        \begin{align}
          a_{\text{new}} = \frac{a - \text{min}_A}{\text{max}_A-\min_{A}} (\max - \min) + \min.
        \end{align}
        Example: let income range from $\$12,000$ to $\$98,000$ normalized to $[0,1]$.\\
        Then $\$73,600$ is mapped to $\frac{73,600-12,000}{98,000-12,000} (1-0) + 0 = 0.716$.
        \item \textbf{Z-score normalization:}
        \begin{align}
          a_{\text{new}} := z(a) = \frac{a-\mu_{A}}{\sigma_A}, \; \text{with $\mu$ being the mean and $\sigma$ the standard deviation.}
        \end{align}
        Example: let $\mu = 54,000$ and $\sigma = 16,000$. Then $\frac{73,000-54,000}{16,000} = 1.188$.
        \item \textbf{Normalization by decimal scaling:}
        \begin{align}
        a_{\text{new}} = \frac{a}{10^k}, \; \text{where $k$ is the smallest integer such that} \; \max(\vert a_{\text{new}} \vert) < 1.
        \end{align}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Discretization}
    \begin{itemize}
      \item \textbf{Three types of attributes:}
      \begin{itemize}
        \item Nominal -- values from an unordered set, e.g. color, profession.
        \item Ordinal -- values from an ordered set, e.g. military or academic rank.
        \item Numerical -- numbers, e.g. integer or real numbers.
      \end{itemize}
      \item \textbf{Divide the value range of a continuous attribute into intervals:}
      \begin{itemize}
        \item \textbf{Interval labels} can then be used to replace actual data values.
        \item Reduce data size by discretization.
        \item Supervised vs. unsupervised.
        \item Split (top-down) vs. merge (bottom-up).
        \item Discretization can be performed recursively on an attribute.
        \item Prepare for further analysis, e.g. classification.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Data-discretization Methods}
    \begin{itemize}
      \item \textbf{Typical methods:}
      \begin{itemize}
        \item All the methods can be applied recursively.
        \item \textbf{Binning:}
              \begin{itemize}
                \item Unsupervised, top-down split.
              \end{itemize}
        \item \textbf{Histogram analysis:}
              \begin{itemize}
                \item Unsupervised, top-down split.
              \end{itemize}
        \item \textbf{Clustering analysis:}
              \begin{itemize}
                \item Unsupervised, top-down split or bottom-up merge.
              \end{itemize}
        \item \textbf{Decision-tree analysis:}
              \begin{itemize}
                \item Supervised, top-down split.
              \end{itemize}
        \item \textbf{Correlation (e.g. $\chi^2$) analysis:}
              \begin{itemize}
                \item Unsupervised, bottom-up merge.
              \end{itemize}
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Simple Discretization: Binning}
    \begin{itemize}
      \item \textbf{Equal-width (distance) partitioning:}
      \begin{itemize}
        \item Divides the range into $N$ intervals of equal size: uniform grid.
        \item If $A$ and $B$ are the lowest and highest values of the attribute, the width of intervals will be: $W = \frac{(B - A)}{N}$.
        \item The most straightforward, but outliers may dominate presentation.
        \item Skewed data is not handled well.
      \end{itemize}
      \item \textbf{Equal-depth (frequency) partitioning:}
      \begin{itemize}
        \item Divides the range into $N$ intervals, each containing approximately the same number of samples.
        \item Good data scaling.
        \item Managing categorical attributes can be tricky.
      \end{itemize}
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Binning Methods for Data Smoothing}
    \begin{itemize}
      \item \textbf{Sorted data for price (in dollars):} \\
            $4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34$.
      \item \textbf{Partition into equal-frequency (equal-depth) bins:}\\
            Bin $1$: $4, 8, 9, 15$,\\
            Bin $2$: $21, 21, 24, 25$,\\
            Bin $3$: $26, 28, 29, 34$.
      \item \textbf{Smoothing by bin means:}\\
            Bin $1$: $9, 9, 9, 9$,\\
            Bin $2$: $23, 23, 23, 23$,\\
            Bin $3$: $29, 29, 29, 29$.\\
      \item \textbf{Smoothing by bin boundaries:}\\
            Bin $1$: $4, 4, 4, 15$,\\
            Bin $2$: $21, 21, 25, 25$,\\
            Bin $3$: $26, 26, 26, 34$.\\
    \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Discretization without using Class Labels (Binning vs. Clustering)}
    \begin{figure}[H]
        \centering
        \begin{minipage}{0.32\textwidth}
            \includegraphics[width=5cm]{img/binningvsclustering2.png}
            \caption{a) Equal interval width (binning).}
        \end{minipage}
        \begin{minipage}{0.32\textwidth}
            \centering
            \includegraphics[width=5cm]{img/binningvsclustering3.png}
            \caption{b) Equal frequency (binning).}
        \end{minipage}
        \begin{minipage}{0.32\textwidth}
            \centering
            \includegraphics[width=5cm]{img/binningvsclustering4.png}
            \caption{c) K-means clustering.}
        \end{minipage}\hfill
    \end{figure}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Discretization by Classification \& Correlation Analysis}
        \begin{itemize}
            \item \textbf{Classification:}
            \begin{itemize}
              \item E.g. decision-tree analysis.
              \item Supervised: Class labels given for training set e.g. cancerous vs. benign.
              \item Using \textbf{entropy} to determine split point (discretization point).
              \item Top-down, recursive split.
              \item Details will be covered in Chapter 6.
            \end{itemize}
            \item \textbf{Correlation analysis:}
            \begin{itemize}
              \item E.g. $\chi^2$-merge: $\chi^2$-based discretization.
              \item Supervised: use class information.
              \item Bottom-up merge: find the best neighboring intervals (those having similar distributions of classes, i.e., low $\chi^2$ values) to merge.
              \item Merge performed recursively, until a predefined stopping condition.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Concept-hierarchy Generation}
        \begin{itemize}
            \item \textbf{Concept hierarchy:}
            \begin{itemize}
              \item Organizes concepts (i.e. attribute values) hierarchically.
              \item Usually associated with each dimension in a data warehouse.
              \item Facilitates \textbf{drilling and rolling} in data warehouses to view data at multiple granularity.
            \end{itemize}
            \item \textbf{Concept-hierarchy formation:}
            \begin{itemize}
              \item Recursively reduce the data by collecting and replacing \textbf{low-level concepts} (such as numerical values for age) by \textbf{higher-level concepts} (such as youth, adult, or senior).
              \item Can be explicitly specified by domain experts and/or data-warehouse designers.
              \item Can be automatically formed for both numerical and nominal data.
              \item For numerical data, use discretization methods shown.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Concept-hierarchy Generation for Nominal Data}
        \begin{itemize}
            \item \textbf{Specification of a partial/total ordering of attributes explicitly at the schema level by users or experts.}
            \begin{itemize}
              \item $\#(\text{streets}) \prec \#(\text{city}) \prec \#(\text{state}) \prec \#(\text{country})$.
            \end{itemize}
            \item \textbf{Specification of a hierarchy for a set of values by explicit data grouping.}
            \begin{itemize}
              \item $\#(\{"Urbana", "Champaign", "Chicago"\}) \prec \#(\text{Illinois})$.
            \end{itemize}
            \item \textbf{Specification of only a partial set of attributes.}
            \begin{itemize}
              \item Only $\#(\text{street}) \prec \#(\text{city})$, not others.
            \end{itemize}
            \item \textbf{Automatic generation of hierarchies (or attribute levels) by the analysis of the number of distinct values.}
            \begin{itemize}
              \item E.g. for a set of attributes: $\{\text{street}, \text{city}, \text{state}, \text{country}\}$.
              \item See on the next slides.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Automatic Concept-hierarchy Generation}
        \begin{itemize}
            \item \textbf{Some hierarchies can be automatically generated based on the analysis of the number of distinct values per attribute.}
            \begin{itemize}
              \item The attribute with the most distinct values is placed at the lowest level of the hierarchy.
              \item Exceptions, e.g. weekday, month, quarter, year.
            \end{itemize}
            \item Example:
            \begin{align}
            \#(\text{streets}) &= 674.339 > \#(\text{city}) =  3567,\\
            \#(\text{city}) &=  3567 > \#(\text{province or state}) =  356,\\
            \#(\text{province or state}) &=  356 > \#(\text{country}) = 15.
            \end{align}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Chapter III: Preprocessing}
        \begin{itemize}
            \item Data preprocessing: an overview.
            \begin{itemize}
              \item Data quality.
              \item Major tasks in data preprocessing.
            \end{itemize}
            \item Data cleaning.
            \item Data integration.
            \item Data reduction.
            \item Data transformation and data discretization.
            \item \textbf{Summary.}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{Summary}
      \begin{itemize}
        \item \textbf{Data quality:} Accuracy, completeness, consistency, timeliness, believability, interpretability.
        \item \textbf{Data cleaning:} E.g. missing/noisy values, outliers.
        \item \textbf{Data integration from multiple sources:}
        \begin{itemize}
          \item Entity identification problem.
          \item Remove redundancies.
          \item Detect inconsistencies.
        \end{itemize}
        \item \textbf{Data reduction:}
        \begin{itemize}
          \item Dimensionality reduction.
          \item Numerosity reduction.
          \item Data compression.
        \end{itemize}
        \item \textbf{Data transformation and data discretization:}
        \begin{itemize}
          \item Normalization.
          \item Concept-hierarchy generation.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{References (I)}
      \begin{itemize}
        \item D. P. Ballou and G. K. Tayi: Enhancing data quality in data warehouse environments. Comm. of ACM, 42:73-78, 1999.
        \item A. Bruce, D. Donoho and H.-Y. Gao: Wavelet analysis. IEEE Spectrum, Oct. 1996.
        \item {\color{airforceblue}T. Dasu and T. Johnson:  Exploratory Data Mining and Data Cleaning. John Wiley, 2003.}
        \item J. Devore and R. Peck: Statistics: The Exploration and Analysis of Data. Duxbury Press, 1997.
        \item H. Galhardas, D. Florescu, D. Shasha, E. Simon and C.-A. Saita: Declarative data cleaning: Language, model, and algorithms. VLDB'01.
        \item M. Hua and J. Pei: Cleaning disguised missing data: A heuristic approach. KDD'07.
        \item {\color{airforceblue}H. V. Jagadish et al.: Special Issue on Data Reduction Techniques.  Bulletin of the Technical Committee on Data Engineering, 20(4), Dec. 1997.}
      \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}[frame number]
    \begin{frame}{References (2)}
      \begin{itemize}
        \item H. Liu and H. Motoda (eds.): Feature Extraction, Construction, and Selection: A Data Mining Perspective. Kluwer Academic, 1998.
        \item J. E. Olson. Data Quality: The Accuracy Dimension. Morgan Kaufmann, 2003.
        \item D. Pyle: Data Preparation for Data Mining. Morgan Kaufmann, 1999.
        \item {\color{airforceblue}V. Raman and J. Hellerstein: Potter's Wheel: An Interactive Framework for Data Cleaning and Transformation, VLDB'01.}
        \item T. Redman: Data Quality: The Field Guide. Digital Press (Elsevier), 2001.
        \item R. Wang, V. Storey and C. Firth: A framework for analysis of data quality research. IEEE Trans. Knowledge and Data Engineering, 7:623-640, 1995.
        \item J. Shlens: A Tutorial on Principal Component Analysis. 2005, URL: \url{https://www.cs.cmu.edu/~elaw/papers/pca.pdf}.
      \end{itemize}
    \end{frame}
  }

  { % Questions?
    \setbeamertemplate{footline}[frame number]
    \begin{frame}[c]
      \begin{center}
        Thank you for your attention.\\
        {\bf Any questions about the third chapter?}\\[0.5cm]
        Ask them now, or again, drop me a line: \\
        \faSendO \ \texttt{luciano.melodia@fau.de}.
      \end{center}
    \end{frame}
  }
\end{document}
